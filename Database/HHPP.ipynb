{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-02X8Objv8FX",
    "outputId": "5396bca3-4699-4d3a-fa1a-5fb2f53f1b1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '$SPARK_VERSION-bin-hadoop3.tgz'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.3.0-bin-hadoop3\\python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\PythonData\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12724\\4027857322.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Start a SparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\PythonData\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    161\u001b[0m             raise Exception(\n\u001b[0;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[1;32m--> 163\u001b[1;33m                     \u001b[0mspark_python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m                 )\n\u001b[0;32m    165\u001b[0m             )\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j in /content/spark-3.3.0-bin-hadoop3\\python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.3'\n",
    "spark_version = 'spark-3.3.0'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0fyakKxwVyH",
    "outputId": "26b24329-a468-41a8-9550-d251ed0ea04e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-11 18:40:39--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n",
      "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
      "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1002883 (979K) [application/java-archive]\n",
      "Saving to: ‘postgresql-42.2.16.jar.1’\n",
      "\n",
      "postgresql-42.2.16. 100%[===================>] 979.38K  4.84MB/s    in 0.2s    \n",
      "\n",
      "2022-09-11 18:40:40 (4.84 MB/s) - ‘postgresql-42.2.16.jar.1’ saved [1002883/1002883]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the Postgres driver that will allow Spark to interact with Postgres.\n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pgyLRdmgwbBQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"M16-Amazon-Challenge\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXEyQMPJwebl",
    "outputId": "d833ed65-d3d3-43dc-a7e5-2a9af25b0c82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+-----------------------+\n",
      "| id|                name|total_population|median_household_income|\n",
      "+---+--------------------+----------------+-----------------------+\n",
      "|  1|    Kashmere Gardens|         10055.0|                23102.0|\n",
      "|  2|Trinity / Houston...|         15789.0|                27789.0|\n",
      "|  3|           Settegast|          2981.0|                28821.0|\n",
      "|  4|Denver Harbor / P...|         17571.0|                29273.0|\n",
      "|  5|          South Park|         21208.0|                29475.0|\n",
      "|  6|            Westwood|         19530.0|                29527.0|\n",
      "|  7|  Greater Fifth Ward|         19687.0|                30535.0|\n",
      "|  8|Clinton Park / Tr...|          3140.0|                31826.0|\n",
      "|  9|        East Houston|         18580.0|                31826.0|\n",
      "| 10|       Magnolia Park|         16999.0|                32039.0|\n",
      "| 11|Gulfgate Rivervie...|         12723.0|                32595.0|\n",
      "| 12|       Pleasantville|          2860.0|                32899.0|\n",
      "| 13|     Eastex - Jensen|         25724.0|                32923.0|\n",
      "| 14|          Sharpstown|         77220.0|                33086.0|\n",
      "| 15|Northside / North...|         59410.0|                33713.0|\n",
      "| 16| Greater Greenspoint|         41392.0|                33909.0|\n",
      "| 17|Greater OST / Sou...|         19141.0|                34019.0|\n",
      "| 18|       Hidden Valley|          3569.0|                34452.0|\n",
      "| 19|          Pecan Park|         16259.0|                34622.0|\n",
      "| 20|Golfcrest / Bellf...|         51432.0|                34731.0|\n",
      "+---+--------------------+----------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "url = \"https://hhpp-bucket.s3.amazonaws.com/HoustonSuperNeighborhoods.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"HoustonSuperNeighborhoods.csv\"), sep=\",\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZv2Rbtv0oUn",
    "outputId": "a3d19854-3357-4e2d-b564-8a64deac4572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter database password··········\n"
     ]
    }
   ],
   "source": [
    "# Store environmental variable\n",
    "\n",
    "from getpass import getpass\n",
    "password = getpass('Enter database password')\n",
    "\n",
    "# Configure settings for RDS\n",
    "\n",
    "mode = \"append\"\n",
    "jdbc_url=\"jdbc:postgresql://hhpp-db.cib8i0dtf6rx.us-east-1.rds.amazonaws.com:5432/hhppdb\"\n",
    "config = {\"user\":\"postgres\", \n",
    "          \"password\": password, \n",
    "          \"driver\":\"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qwGxn4MW1-9J"
   },
   "outputs": [],
   "source": [
    "# Write df to table in RDS\n",
    "df.write.jdbc(url=jdbc_url, table='houston_super_neighborhoods', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPiXgld35twa",
    "outputId": "6885a64a-b65f-4ebb-b7c2-06b64273b330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+-----------------------+\n",
      "| id|                name|total_population|median_household_income|\n",
      "+---+--------------------+----------------+-----------------------+\n",
      "|  1|    Kashmere Gardens|           10055|                  23102|\n",
      "|  2|Trinity / Houston...|           15789|                  27789|\n",
      "|  3|           Settegast|            2981|                  28821|\n",
      "|  4|Denver Harbor / P...|           17571|                  29273|\n",
      "|  5|          South Park|           21208|                  29475|\n",
      "|  6|            Westwood|           19530|                  29527|\n",
      "|  7|  Greater Fifth Ward|           19687|                  30535|\n",
      "|  8|Clinton Park / Tr...|            3140|                  31826|\n",
      "|  9|        East Houston|           18580|                  31826|\n",
      "| 10|       Magnolia Park|           16999|                  32039|\n",
      "| 11|Gulfgate Rivervie...|           12723|                  32595|\n",
      "| 12|       Pleasantville|            2860|                  32899|\n",
      "| 13|     Eastex - Jensen|           25724|                  32923|\n",
      "| 14|          Sharpstown|           77220|                  33086|\n",
      "| 15|Northside / North...|           59410|                  33713|\n",
      "| 16| Greater Greenspoint|           41392|                  33909|\n",
      "| 17|Greater OST / Sou...|           19141|                  34019|\n",
      "| 18|       Hidden Valley|            3569|                  34452|\n",
      "| 19|          Pecan Park|           16259|                  34622|\n",
      "| 20|Golfcrest / Bellf...|           51432|                  34731|\n",
      "+---+--------------------+----------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from table\n",
    "read_df = spark.read.jdbc(url=jdbc_url, table='houston_super_neighborhoods', properties=config)\n",
    "read_df.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
